[<](../README.md)

# Identify the core AWS services

## Describe the categories of services on AWS (compute, storage, network, database)

## Identify AWS compute services


At AWS, virtual servers are called **Elastic Compute Cloud (EC2) instances**.

At the most basic level, each instance is a virtual server running a version of Windows or Linux. Intances run as a guest operating system hosted by the hypervisor.


Each instance is configured with a select amount of: memory, vCPU cores, storage and networking bandwidth. Components part of each configuration, regardless of the instance size and type:  
- Built by preconfigured images called Amazon Machine Images (AMI)
- Authenticated using a unique public/private key pair per instance
- Storage options of persistent block storage and temporary storage volumes
- A mandatory firewall called a Security Group that protects your network interfaces
- Basic or enhanced network adapters
- Shared or dedicated hardware options


#### *Amazon Machine Images (AMIs)*

**All** instances are created using an AMI. An AMI is a template that contains the desired software configuration; an operating system and a root device boot volume at a minimum; and optionally, an application and perhaps some additions supporting software

Each AMI contains the necessary technical information required to launch all instances hosted at AWS.

Each amy has 6 components:  
1. **Boost volume:** Describes what will be used as the root boot volume for the instance.
2. **Launch permissions:** Define the AWS accounts that are permitted to use the AMI to launch instances with. *Default launch permissions are set to private*, which means that only the owner who created the AMI can use it.
3. **Volumes to attach:** The volumes to attach to the EC2 instance at launch are contained in a block device mapping document
4. **Default location:** AMIs are region specific. It is stored in the region where it was initially created. However, can be made available in other regions by copying them to the desired region.
5. **Operating system:** Linux or Windows 
6. **Root device storage**: There are two options available: either backed by Amazon EBS (EBS snapshot stored in S3) or backed by an instance store volume (template stored in S3)

**OBS:** There are many prebuilt AMIs available in the Amazon Marketplace and in the EC2 management console. But, you can create a custom AMI after an instance has booted.

**OBS:** AMI best practices:
- Don't embed passwords or security information in the AMI; instead *use IAM roles and AWS Secrets*
- Use a standard bootstrapping process with user data scripts
- Properly tag your AMIs for identification purposes.  Tags are used for monitoring, automation, security, and billing

### **Recognize there are different compute families**

Instances are members of several **compute familities**, whereby instances are grouped and defined by a name and generation designation. For each instance. Ex: `c4d.4xlarge`
1. The first letter is the instance family that it belongs to (`c` => stands for compute)
2. The next number is the generation number (`4` => "version number")
3. Its call a feature and define the key characteristics of the instance (`d` => denote solid state drives (SSD) for instance storage)
4. Size of the instance based on the amount of vCPU cores, the amount of RAM, and the amount of allocated network bandwidth (`4xlarge` => 4 times larger than the c4.large)

 Once an instance has been ordered, the resources (vCPUs, memory, and network bandwidth) allocated are assigned to your account and are never shared with any other AWS customer => **there is no common pool of compute resources constantly being shared among customers**.

**OBS:** Customers are virtually isolated from each other, and this isolation forms a key element of security.

**OBS:** AWS defined the amount of CPU power assigned to each instance as a virtual CPU (vCPIU). A vCPU is a part of a physical CPU core.


#### *EC2 Instance Choices*

After selecting an AMI, you choose the instance type where the AMI will be installed.

Properly sizing the selected instance to match application needs is of key importance when running production workloads at AWS.

There are more than 150 instance choices available at AWS, from general purpose to instances designed for computer, storage, and memory-optimized workloads.


1. **General Purpose Instances:** provide a baseline of CPU processing, network, and storage.
   - Some, such as T2 and T3, are designed to burst. Using CPU credits that the instance can earn while it is not running under load, they can burst above their initial CPU baselined as required.
   - If you run out of CPU credits, your CPU performance is reduced to the assigned CPU baseline based on the type of T instance you are running
  
**OBS:** Using a T instance for applications that require maximum CPU performance for unknown lengths of time is not a good design idea.

**OBS:** AWS CloudWatch has available metrics allowing you to monitor the credits being used and the current credit balance.

2. **Compute-Optimized Instances:** designed for batch processing workloads, media transcoding, and high-performance applications or Web servers. 

3. **Memory-Optimized Instances:** designed for workloads that need to process bast data sets hosted in memory, such as MySQL and NoSQL databases, or Memcached and Redis in-memory cache stores.

4. **Accelerated Computing (GPU) Instances:** designed for high performance with NVIDIA GPUS, for graph processing, (`accelerated computing instances` and `g3 instances`) or hardwared accelaration image (FPGA Image) combined with custom AMI, used for financial computing, genomics, accelerated search, and image-processing applications.

5. **Storage-Optimized Instances:** designed for workloads that require local SSD storage for large data sets. Provide high sequential read and write access with high IOPS performance due to the local storage

6. **Bare-Metal Instances:** the bare-metal platform at AWS is used to host the VMware cloud on AWS.


#### *Dedicated Hosts*

Choosing a dedicated host involves choosing a dedicated physical server with a defined EC2 instance capacity just for you.

Allows controling:
- the hardware the instances are running on
- the *affinity*, or placement, of your instances, on the host you desire

Support for *per-socket*,*per-core*, or *per-VM* software licenses.

Limitations:
- The instance size and type of instance placed on a dedicated host must be the same type.
- Red Hat Enterprise Linux and Windows AMIs cannot be used with dedicated hosts. Only  bring your own license (BYOL) Amazon Linux and Amazon Marketplace AMIs.
- Instances hosted on a dedicated host must be launched in a VPC
- Amazon relational database service (RDS), placement groups, and EC2 auto-scaling groups are not supported

#### *Dedicated Instances*

Each dedicated instance runs in a VPC on dedicated hardware resources. 

Limitations:
- You are not allowed to target your instance’s placement
- No access or control of the sockets and physical cores of the physical host is allowed
- EBS volumes that are attached to a dedicated instance are not running on single-tenant hardware; EBS storage is a shared storage array

#### *Launch Templates*

Launch templates can be created that contain the configuration details you wish to use for EC2 instances. 

Some of the parameters that can be used in launch templates are:
- **Image creation:** AMI ID, isntance type, key-pair name, the VPC subnet, and security group
- **Networking information:** create up to two additional network interfaces, subnet locations, public and private IP addresses, and assigned security groups
- **Storage volumes:** Instance store or EBS volumes, size, IOPS, encryption, and tags
- **Advanced options:** Purchasing options, IAM profile, shutdown behavior, termination protection, detailed monitoring, tenancy, and user data

#### *EC2 Instance Storage Options*

There are 3 main types of storage that instances can directly utilize:  
1. Persistent block storage volumes (elastic block storage, or EBS)
2. Temporary block storage, or ephemeral storage
3. Shared file storage options, such as the elastic file system, Amazon FSx, and S3 block file storage

#### *EC2 Auto Recovery*

On modern instances (C3, C4, C5, M3, M4, R3, R4, T2 and X1 families) that have only EBS volumes attached, you can take advantage of a feature called **EC2 auto recovery**.

Once enabled, in the background the monitoring service CloudWatch has permitted an alarm with an instance metric called `StatusCheckFailed_System`. 

If your instance fails, the defined recover instance workflow goes into action. The failed instance is launched on different hardware and retains its instance ID, the instance metadata, private IP and elastic IP addresses, and the EBS volumes that were previously attached.


#### *Amazon LightSail*

LightSail allows you to quickly build an application stack from a preconfigured blueprint installed on an instance. Current choices includes:  
- Application stack
- Databases
- Networking resources
- Storage
- Snapshot

### **Recognize the different services that provide compute** 

There are other compute choices to be aware of. It all depends on the type of application stack you’re trying to build or host

#### *Containers*

Another way of running applications at AWS is by using containers.

The concept of **containers** involves changing the underlying virtualization from a complete emulation of a computer system
to just the virtualization of the operating system components that are required for communication: **the necessary libraries, system tools, and runtime**.

Types of container services:  
1. **Amazon Elastic Container Service (ECS):** Complete container management system supporting Docker containers and Windows Server containers that is certified conformant with Kubernetes, allowing you to use all third-party plug-ins and customizations from the Kubernetes community.   
   - ECS allows you to set up a cluster of EC2 instances running Docker in a selected VPC; it then manages the deployment of the containers. 
   - Containers are launched using a task definition that defines the following criteria:
     - The Docker image to be pulled from the private registry for the container
     - The CPU and memory requirements
     - Links that need to be established between containers
     - Data volumes
     - Network and port settings
     - IAM security settings
   - The ECS follows what is called a **task placement strategy** using the defined task definition when launching your containers
   - ECS is fully integrated with the monitoring services at AWS (CloudTrail), load-balancing services (ALB), role-based security (IAM), and your own private network (VPC).

2. **AWS Fargate:** Fargate is a container management service that allows you to run your applications in containers at AWS but not have to manage the underlying instances and clusters.
   - You can specify the container image and memory and CPU requirements using an ECS task definition. Then, Fargates takes over  scheduling the orchestration, management, and placement of the containers throughout the cluster 
3. **AWS ECS for Kubernets (EKS):** Similar to the concept of AWS Fargate container management, Amazon deploys and hosts the Kubernetes control plane, which controls clusters of worker nodes hosted across multiple AZs.
   - EKS has been certified Kubernetes conformant, so any third-party plug-ins or tools that you are using or have developed can be migrated to AWS
   - AWS is also integrated with Elastic Load Balancing (ELB), IAM security services for authentication, VPC hosting for isolation, and AWS CloudTrail.

#### *AWS Lambda*

**AWS Lambda** is a manage AWS service that allows a customer to craft custom functions to carry out any task written in a variety of programming languages. Lambda is available as a hosted service at each edge location.

Instead of spinning up your own instance, consider uploading and executing  tasks as Lambda functions or as Lambda functions to Amazon’s hosted instances.

Your concern is your custom Lambda function; Amazon’s job is hosting and executing your function.

How it works?  
1. Upload your functions to Lambda as a zip file
2. Define how long the function will execute
3. Define how much compute and memory the function requires
4. Define the trigger that starts the funciton


### **Recognize that elasticity is achieved through Auto Scaling**

Real-time monitoring of a hosted cloud application allows AWS to react almost instantaneously before the application's performance is close to degrading. 

**EC2 Auto Scaling** > Additional computer resources are automatically ordered and delivered to the application server's cluster maintaining the server performance.

In a nutshell, EC2 Auto Scaling makes sure that the compute power required by your application is always available.

EC2 instances are grouped together in what is called an **Auto Scaling
group (ASG)**. Once you define the minimum number of instances for each ASG, auto scaling ensures that the minimum defined instance number is always available. Other options are defining a maximum number of instances in the ASG and  maintaining a desired capacity of compute instances;

**OBS:** The desired compute capacity can be scaled out or in based on CloudWatch alarm triggers, which can order a change in the number of instances plus or minus. If a desired capacity value is entered, the desired capacity is maintained.

Additional benefits of ECS auto scaling:  
- **Cost management:** EC2 auto scaling helps to save money; if your application has just the right amount of capacity—not too much and not too little—then you’re not wasting compute power that you’re not utilizing
- **Fault tolerance:** EC2 auto scaling integrates with AWS load balancing services (ELB) and AZs. Auto scaling makes the load balancer’s job easier because the resources available in each AZ can be scaled up and down based on demand.  
- **Health Checks:** EC2 auto scaling can detect when instances are unhealthy and remove and replace the instance that’s unhealthy with a new instance

Auto Scaling Components:
1. **Launch Configuration:** The launch configuration is a template used by the ASG to launch EC2 instances. 
  - The template includes numerous system components, including the instance ID of the AMI, the instance type to build, the key pair for authentication, desired security groups, and a block storage device
2. **Launch Templates:** A launch template is similar to a launch configuration, with the added feature of **versioning**
3. **Auto Scaling Groups (ASGs):** An Auto Scaling group (ASG) is built from a collection of EC2 instances that have been generated from the associated launch configuration or launch template
     - Each ASG launches instances following the parameters of the launch template to meet the defined scaling policy
     - ASGs perform health checks on instances added to an ASG 
       - Auto Scale health checks are checking availability of the instance using the EC2 status check. If instances added to an ASG fail their status checks after boot, they are considered unhealthy, terminated, and re-added to the ASG
     - Scaling Options for ASGS:
       - **Manual scaling:** You can make manual changes to your ASGs at any time by changing the maximum, minimum, or desired state values to start capacity changes
       - **Maintain current instance levels:** Set the desired capacity and health checks to determine the health and automatically replace any instances that are determined to be unhealthy 
       - **Target tracking:** Increase and decrease the ASG based on a metric
       - **Scale based on a schedule:** caling can also be defined by time and date values, which instructs auto scaling to scale up or down at a specific time
       - **Scale on demand policy:** Demand scaling allows users custom metrics that will determine the scaling out and in of your defined fleet of EC2 instances. Multiple policies can be attached to an ASG that can both control the scaling out and in.

**OBS:** AWS Auto Scaling => management tool for scaling across
application stacks using the familiar ELB service for load-balanced distribution of EC2 compute instances => AWS Auto Scaling rests on  **predictive scaling** and uses machine learning models
to analyze the applications, and traffic pattern history to forecast potential changes in the future. Predictive scaling currently only works with EC2 instances


### **Identify the purpose of load balancers**

Operating in the AWS cloud,  AWS services, but EC2 Instances, are offered with a standard level of redudancy. Redundancy for your EC2 compute resources at AWS is accomplished by having additional instances or containers and placing these resources behind a load balancer

Amazon's Elastic Load Balancing (ELB) service is a large redundantly hosted regional service that can scale on demand; as your load balancer handles more request in the background, the ELB scales up to meet the demand.

**ELB can be a key component in your overall application design stack to ensure a highly available and redundant application**. If the load-balancing components that ELB assigns to you fail, you are
automatically failed over to another load balancer within the massive regional server farm that makes up ELB. 

The ELB service can distribute the incoming application requests to EC2 instances, containers, and IP addresses hosted in a single AZ or multiple AZs.

**OBS:** EC2 Health Checks => Each load balancer performs health checks at a specific interval to all registered instances. If the instance’s response exceeds what’s defined as the healthy threshold for consecutive responses, the instance is considered unhealthy and is taken out of service.

Additional ELB Features:  
- **SSL/TLS Traffic Decryption:** The ALB supports SSL offload, performing SSL/TLS decryption on the incoming connection request, which terminates the SSL traffic on the load balancer before sending the decrypted requests to the application
- **IPv4 or v6 support:** The IP address type for your Internet-facing load balancer supports both IPv4 and IPv6 addresses; internal load balancers must use IPv4 addresses
- **Routing:** Using the ALB, content-based, path-based, and host-based routing are supported, allowing connection requests to individual services contained within a domain or container based on the type of the request
- **Dynamic port mapping:** The ALB supports load balancing containers running the same service on the same EC2 instance where the containers are hosted
- **Connection draining:** Once an EC2 instance that is registered with a load balancer is tagged as unhealthy by failing its health checks, the connections to the instance are closed through a process called connection draining.
  - *Connection draining* keeps existing connections open until the client closes them but prevents new requests from being sent to the instances that are tagged as unhealthy
- **Cross AZ Support:** Load balancers distribute the incoming traffic requests evenly across the associated AZs.
  - You are charged for data transfers between AZs when cross-zone load balancing is enabled
- **User authentication:** The ALB allows you to offload the authentication process so it can authenticate users as they request access to cloud applications


Monitoring Load Balancer Operation:  
- **CloudWatch:** ELB metrics can be used to monitor and ensure your system is performing as you expected. Only when your load balancer is responding to user requests will metrics be reported to CloudWatch for analysis 
- **Access logs:** Provide detailed information about all requests sent to your load balancer. Once enabled, ELB captures the logging detail and stores it in the desired S3 bucket.

AWS offers three options for load balancing: the Application Load Balancer (ALB), the Network Load Balancer (NLB), and the Classic Load Balancer (CLB). However, the CLB is only available due to its continued use by existing AWS customers

1. **Application Load Balancer (ALB):** designed for the load balancing of containers, instances, and private IPaddresses using target groups
   - A *target group* is the destination where the listener sends incoming requests. Targets include instances, containers, and IP addresses.
   - Target groups are defined by target type:  
     - Instance (defined by instance ID)
     - IP (defined by private IP addresses)
     - Lambda (defined by a specific Lambda function)
   - Load balancers communicate using a process called a **listener**, which continually checks for any incoming requests based on the defined protocol and port that you have configured.  
   - Listeners follow **rules** that determine which target group the incoming connection requests are sent to 
   - Rules are precise actions to be carried out by the listener the rules are assigned to. 
   - Actions that can be carried out using rules:
     - forward: send requests to the named target group
     - redirect: redirect the request from one URL to another
     - authenticate-cognito: AWS Cognito is used to authenticate users
     - authenticate-oidc: OpenID Connect authentication
     - fixed-response: send a custom HTTP response to the end user
     - host-based routing: This routing option forwards requests to a target group based on the domain name contained in the host header
     - path-based routing: If the path in the URL matches the path pattern defined in the listeners rule, the request is routed
   - ELB **supports sticky sessions** that allow the load balancer to bind the user’s active session to an instance


2. **Network Load Balancer (NLB):** designed for **private access**. One of the primary reasons for NLBs at AWS is the support of VPC Endpoint Services, also called AWS PrivateLink, which use NLBs in front of private application stacks hosted in VPCs.

**OBS:** The gang of three: CloudWatch, Auto Scale, and ELB (load-balancing) => One of the best example of AWS service integration and automated problem-solving is the relationship between the gang of three essential AWS services: CloudWatch, Auto Scale, and ELB (load-balancing), which can work together seamlessly: 

1. Instances hosted on subnets in different availability zones (AZs) can be protected by a **load balancer**. The instances can be monitored by CloudFront on a select metric such as network packets in or CPU utilization.  
2. Once the selected metric on the instances has exceeded the defined performance threshold for a defined period, **CloudWatch** can promptly fire an alarm that calls the auto-scaling service.  
3. **Auto Scale** starts the build of an additional instance; once ready, the instance is added to the pool of instances targeted by the load balancer. 


## Identify different AWS storage services
### **Describe Amazon S3**
### **Describe Amazon Elastic Block Store (Amazon EBS)**
### **Describe Amazon S3 Glacier**
### **DescriCbe AWS Snowball**
### **Describe Amazon Elastic File System (Amazon EFS)**
### **Describe AWS Storage Gateway**



## Identify AWS networking services

### **Identify VPC**

Networking at AWS is called the **virtual private cloud (VPC)**. The official name of a VPC is **EC2-VPC**. The **EC2 (Elastic Cloud Compute)** designation indicates that EC2 instances are usually hosted within a VPC. 

Each VPC is a logically isolated "data center" where computer instances and various AWS services reside.

**OBS:** Amazon job when a customer order a VPC:
- Amazon must make sure that my VPC is a private isolated software data center linked to my AWS account. 
- Amazon must ensure the continued separation of my VPC from all other customers. 
- Amazon also guarantees that network traffic is routed and protected based on my architecture design and needs.
- Im sum, **AWS's job is to provision, host, and secure the VPC**.

AWS services can be **hosted within**, or **associated to** a VPC. This means that some of the services will have relationships with the services and components hosted *inside* a VPC, but the services themselves are not actually *installed in* the VPC.

Examples of Hosted VPC Services: 
- EC2 instances
- Elastic Beanstalk
- Amazon Redshift
- ElastiCache
- Amazon EMR
- Amazon RDS
- Amazon Workspaces

Examples of Associated Services
- Trusted Advisor
- AWS Config
- VPN connections
- Auto scaling
- Elastic load balancing
- S3
- DynamoDB


VPCs run on top of the physical AWS network infrastructure that a customer don't have access. Within each VPC, the networking exposed to each customer is designed and managed at the subnet level.

There are three ways for creating a VPC:
- Create VPC Dashboard
- Launch VPC Wizard
- Amazon Command-Line Interface (CLI)

There is a default "soft limit" to how many VPCs a customer can initially create: 5 VPCs. A customer can request additional VPCs resources up to a defined "hard limit": *current number of VPCs in the region* * *number of security groups per VPC* (this cannot exceed 1000).

VPCs are originally a blank slate except for the primary **IPv4 CIDR block** and the local **main routing table**. IPv6 CIDR blocks can also be associated with a VPC, but only after an  initial IPv4 CIDR block has been created. 

The primary IPv4 CIDR block that a customer choose for the VPC will determine the number and size of IPv4 addresses that can be assigned to the subnets created within the VPC. 

The initial CIDR block added when creating a VPC can't be changed; however, a customer has the option of adding additional secondary CIDR blocks to an existing VPC.

Up to four secondary IPv4 CIDR blocks can be associated with an existing VPC. The additional secondary CIDR block cannot be larger than the primary CIDR block.

Vantage of being able to add additional secondary CIDR blocks to an existing VPC: the ability to add future expansion when necessary. 

Config of default VPC:
- IPv4 CIDR block of 172.30.0.0/16, which provides up to 65531 private IP v4 addresses
- Internet gateway with a route table entry
- Default security group
- Default network ACL

#### *Subnets*

After initial VPC creation, the next step is to create subnets within the VPC, per AZ(s), depending on required network design. Each subnet that a customer create resides within its assigned AZ.

**OBS:** AZs don't show up automatically in each VPC; they are added during subnet creation when selecting each subnet's location.

Subnets are defined by their connectivity options
- **public subnet**: subnet traffic is routed to the internet through an Internet gateway
- **private subnet**: the subnet has no gateway or endpoint to direct traffic to. 

Summary of subnets:
- Subnets are contained within an AZ
- Subnets host compute instances
- Public subnets allow you access to the Internet
- Public subnets are for infrastructure
- Private subnets are private with no Internet access
- Private subnets are where instances live privately
- VPN-only subnets have access to A VPN connection

**OBS:** At AWS, a NAT gateway service can be ordered and linked to a public subnet to allow EC2 instances in a private subnet to connect to the internet and receive required operating system and application updates.

#### *Route tables*

A subnet **route table** is a defined collection of rules that dictate where egress subnet network traffic can flow to.

The main route table provides local routing services throughout the VPC across all defined AZs.

Each route table has an entry containing the VPC's initial CIDR designations, and this entry cannot be changed.

It is considered a best practice to create **custom route tables** for each tier in your network design. Creation of custom route tables allows you granular control of traffic flow within each subnet.


#### *Private vs. Public vs. Elastic IPv4 Addresses*

**Private IP addresses** communicate across the private network at AWS. When a client launches a new EC2 instance, by default, Amazon assigns a private IP address. If you choose to manually assign a primary private IP address, the IP address chosen must be available in the subnet’s IP address range

**OBS:** Once a primary private IP address is assigned, the EC2 instance retains the address for the lifetime of the instance. Additional, secondary private IP addresses can also be assigned to the network interfaces of an EC2 instance, and these addresses can be  unassigned and moved between other EC2 instances that reside on the same subnet at any time

**Public IP addresses** are an option for the customer, are not available by default, and are not permanently assigned to an EC2 instance. 

**OBS:** Public IP addresses from AWS’s own pool are managed and controlled by AWS. To make sure that AWS doesn’t run out of addresses, the AWS-assigned public IP address is detached when an EC2 instance is turned off.

A **public elastic IP address (EIP)** is a static public IP address that after creation is assigned to your account at AWS.

Therefore, at AWS, there are two public pools of IP addresses to consider: Dynamically assigned public IP addresses that are returned to the common public pool of AWS addresses, or elastic IP addresses, which are assigned as desired after ordering and assigned to a customer’s account.


#### *Peering VPCs*

When a company has multiple VPCs in many AWS accounts, we can create networking connections between VPCs through a process called **peering**, enabling you to route traffic between VPCs that have been peered together.

Peering can be carried out between your own account VPCs or between a VPC assigned to another AWS account. Peered VPCs can also reside in completely different regions


### **Identify security groups**

Security groups operate at the EC2 intance level and protect your server's incoming network connections by placing a software firewall around each attached network interface.

Each security group **is a collection of inbound and outbound rules that designate the port and protocols allowed into and of each network interface**. It works as a reusable security template stored in your AWS account. Once a security group has been created, it can be assigned multiple times within the VPC where it was created, protecting one or many EC2 instances.

Security groups summary:
- Security groups are associated with a particular VPC.
- Each network interface assigned to an instanced hosted withing a VPC can be associated with up to five security groups.
- Security groups define only allow rules
- Security groups don’t deny traffic explicitly; instead, they deny traffic implicitly by only stating what is allowed
- Security group rules allow you to direct traffic outbound from one security group and inbound to another security group within the same VPC
- Security groups are defined as stateful: if requests are allowed in, response traffic is allowed out regardless of defined outbound rules
- For each rule you define: protocal, port, port range, and source of inbound rules or destination outbound rules for the traffic
- Protocols allowed: TCP, UDP, or ICMP

**OBS:** If you don’t pay attention and don’t specify a custom security group when an EC2 instance is created, at launch, the default security group is associated automatically.

We can create custom security groups by establishing:
- **Inbound rules:** Define the source of the traffic—that is, where it is coming from, and what the destination port or port range is.
- **Outbound rules:** Define the destination of the traffic—that is, where it is going to, and the destination port, or port range

#### *Gateway VPC Endpoints*
VPC endpoints are defined as a gateway, or interface endpoint.

VPC endpoints allow you to connect VPC subnets to AWS services without  requiring an Internet gateway, VPN connection, or a peering connection be set up first.

Endpoints are another AWS service designed as a horizontally scaled redundant, highly available service.

VPC endpoints are controlled by an endpoint policy, which is also a  IAM (identity and access management) resource. IAM policies define the exact permissions allowed or denied.


### **Identify the purpose of Amazon Route 53**

As a customer access an application hosted at AWS, an application query will be carried out. This is accomplished using DNS (Domain Name System).

Route 53 is Amazon's hosted DNS service and it's highly scalable and highly available. Its primary job is to route end users to hosted applications at AWS.

Route 53 has a public side pointed to the public edge locations that accepts incoming customer requests and then resolves each query to the requested AWS resource located on Route 53's private side.

Route 53 supports a variety of routing protocols, including: 
- round robin (WRR): WRR allows you to assign weights to resource record sets
- latency-based routing (LBR): LBR is utilized when you are balancing application performance across multiple regions
- Geo DNS:  allows you to balance user requests to specific endpoints based on the geographical location the user request came from.

Route 53 also offers health checking and traffic-steering services for routing selected traffic to a healthy endpoint in multiple AWS regions

Route 53 health checks constantly verifies that your applications are reachable and available. Health checks are supported for HTTPS, HTTP, and TCP.

Health checks can also be carried out using the built-in monitoring service CloudWatch using many defined metrics.


### **Identify VPN, AWS Direct Connect**

A **VPN connection** is a site-to-site tunnel connection between your VPC and your corporate network. 

The purpose of **AWS Direct Connect** is to connect your internal corporate network to a Direct connect (DC) location over a private high-speed fiber connection with no Internet connection.